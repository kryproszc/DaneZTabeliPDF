{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm-KcLyGmDTx"
      },
      "outputs": [],
      "source": [
        "#zadaniem jest dopasowanie rozkladow do danych rzecyzwistych.\n",
        "#Rozklady sa tez definiowane troszke w inny sposob niz takie klasyczne, ze studiow.\n",
        "#Poniżej dopasowałem jeden z rozkładów (akurat przesyniety logarytmiczno normalny).\n",
        "#Na danych z tego rozkładu chciałbym byśmy sie oparli\n",
        "# tzn. do tych danych będziemy dopasowywali inne rozklady\n",
        "\n",
        "#W dopasowaniu wzorowałem sie dotychczas na\n",
        "https://github.com/rickecon/Notebooks/blob/master/MLE/MLest.ipynb\n",
        "# wykorzystując kod opisany od 1 do 4. z tym, że ja musze skorzystac z algorytmu BFGS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import lognorm\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "vNFUqgdMmV7T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dane, do ktorych bedziemy sie dopasowywali"
      ],
      "metadata": {
        "id": "_Owc7ihpudw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc_df = lognorm.rvs(s=1.5586,loc=1341909,scale = np.exp(13.3698),size = 1000)"
      ],
      "metadata": {
        "id": "cl_hVRl7mtYQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#czyli zakladamy, ze nasze dane wejsciowe to sc_df (powyżej)"
      ],
      "metadata": {
        "id": "r331cqQqogbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chcemy teraz dopasować Burr Distribution (korzystałęm z tej samej definicji, co wysłałem we wcześniejszej wiadomości).\n",
        "#Parametry zainicjowane zawsze mam podane, taki teraz podaje\n",
        "alpha = 2.8428\n",
        "theta = 1559643\n",
        "gamma = 1"
      ],
      "metadata": {
        "id": "Y1Jkw8MctdvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dodatkowo rozklad jest truncated w przedziale\n",
        "[1341909,100000000000]"
      ],
      "metadata": {
        "id": "didkV902wZOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#maksymalna ilosc symulacji to 10 000"
      ],
      "metadata": {
        "id": "INIVq9nluMhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parametry jakie powinnismy otrzymac to (w sensie w tych okolicach)\n",
        "#czyli znacznie wieksze alpha i theta niż te zainicjowane.\n",
        "alpha = 82\n",
        "theta = 10 122 284 281\n",
        "gamma = 0.323"
      ],
      "metadata": {
        "id": "ALGgBZbYukQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "import numpy as np\n",
        "\n",
        "# Nasz rozkład Burra\n",
        "def custom_burr_cdf(x, alpha, gamma, theta):\n",
        "    return 1 - (1 / (1 + (x / theta)**gamma))**alpha\n",
        "\n",
        "# Wyznaczamy gęstość\n",
        "def custom_burr_pdf(x, alpha, gamma, theta):\n",
        "    return (alpha * gamma / theta) * ((x / theta)**(gamma - 1)) / (1 + (x / theta)**gamma)**(alpha + 1)\n",
        "\n",
        "# Negative log-likelihood\n",
        "def neg_log_likelihood(params, data):\n",
        "    alpha, gamma, theta = params\n",
        "    ll = -np.sum(np.log(custom_burr_pdf(data, alpha, gamma, theta) + 1e-9))\n",
        "    return ll\n",
        "\n",
        "\n",
        "# Parametry startowe\n",
        "initial_params = [2.8428, 1, 1559643]\n",
        "\n",
        "# Optymalizacja\n",
        "result = minimize(neg_log_likelihood, initial_params, args=(sc_df,), method='L-BFGS-B',\n",
        "                  bounds=[(0.01, None), (0.01, None), (0.01, None)])\n",
        "\n",
        "alpha_opt, r_opt, theta_opt = result.x\n",
        "\n",
        "print(\"Fitowane parametry:\")\n",
        "print(\"alpha:\", alpha_opt, \"gamma:\", r_opt, \"theta:\", theta_opt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v73gKARMjKYF",
        "outputId": "1458364c-68c1-4cfe-80c9-2144f99e288d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitowane parametry:\n",
            "alpha: 0.20713328781837384 gamma: 10.4753316935854 theta: 1559642.9999287522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Jeli zna Pan jakiś sposób inny niż w tym artykule na githubie to bardzo bym prosił o pomoc\n",
        "#Mega bym prosił\n",
        "#wiadomo, nie musza wyjść tak idelanie tyle, bo to dane, do których dopasowujemy sa inne niż rzeczywiste (w sensie już po dopasowaniu)\n",
        "#ale ta metoda z githuba pokazuje, że wykonuje 16-20 iteracji tylko."
      ],
      "metadata": {
        "id": "Nrj8iYyLu-ED"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}